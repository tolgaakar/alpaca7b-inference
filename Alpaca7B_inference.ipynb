{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basic inference with the quantized version of Alpaca7B model from Huggingface Hub"
      ],
      "metadata": {
        "id": "1qzHSe3Za1IA"
      },
      "id": "1qzHSe3Za1IA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required libraries"
      ],
      "metadata": {
        "id": "zUhrjR2ZbF8n"
      },
      "id": "zUhrjR2ZbF8n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A specific version of the Transformers library is needed to access LLaMA related imports"
      ],
      "metadata": {
        "id": "JWY5sxplanaQ"
      },
      "id": "JWY5sxplanaQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34bbf16-ef56-4e8b-9064-e20764b09c5b",
      "metadata": {
        "tags": [],
        "id": "f34bbf16-ef56-4e8b-9064-e20764b09c5b",
        "outputId": "02d0079c-56d3-4bb2-d40a-a55ef5c46943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m  WARNING: Did not find branch or tag 'c3dc391', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip -q install git+https://github.com/huggingface/peft.git\n",
        "!pip -q install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model and the tokenizer"
      ],
      "metadata": {
        "id": "LMAHcTkzbMwr"
      },
      "id": "LMAHcTkzbMwr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The quantized ALpaca7B model takes up approximately 8GB in the GPU RAM"
      ],
      "metadata": {
        "id": "iY8B2gnhbmz1"
      },
      "id": "iY8B2gnhbmz1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eddfb0e5-e98d-4a78-8477-0d20bf74ff03",
      "metadata": {
        "tags": [],
        "id": "eddfb0e5-e98d-4a78-8477-0d20bf74ff03"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c270e993-04d0-404a-8a02-0a2fdab0ef4e",
      "metadata": {
        "tags": [],
        "id": "c270e993-04d0-404a-8a02-0a2fdab0ef4e",
        "outputId": "8dc68a98-cc4b-4f55-c6b4-4f232cffb373"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
            "The class this function is called from is 'LLaMATokenizer'.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = LLaMATokenizer.from_pretrained(\"chainyo/alpaca-lora-7b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0bc583f-136e-48bb-80fc-025ff4f47395",
      "metadata": {
        "tags": [],
        "id": "d0bc583f-136e-48bb-80fc-025ff4f47395",
        "outputId": "c1491959-75e4-4932-a6e2-b2128ea0f491",
        "colab": {
          "referenced_widgets": [
            "5322c949715542c3b929c3978468d21c"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 6.0\n",
            "CUDA SETUP: Detected CUDA version 116\n",
            "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/conda/bin/jupyter lab')}\n",
            "  warn(msg)\n",
            "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/jovyan/Untitled.ipynb')}\n",
            "  warn(msg)\n",
            "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
            "  warn(msg)\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5322c949715542c3b929c3978468d21c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/39 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = LLaMAForCausalLM.from_pretrained(\n",
        "    \"chainyo/alpaca-lora-7b\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bug report and warnings above can mostly be ignored"
      ],
      "metadata": {
        "id": "q-biA951adQt"
      },
      "id": "q-biA951adQt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b8921f-ee23-424c-a900-2fde1683838e",
      "metadata": {
        "tags": [],
        "id": "84b8921f-ee23-424c-a900-2fde1683838e"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "if torch.__version__ >= \"2\":\n",
        "    model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the standart Alpaca prompt"
      ],
      "metadata": {
        "id": "imcmCbDebQwH"
      },
      "id": "imcmCbDebQwH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ddf0c38-4968-4610-a7f6-25d9a519042b",
      "metadata": {
        "tags": [],
        "id": "6ddf0c38-4968-4610-a7f6-25d9a519042b"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(instruction: str, input_ctxt: str = None) -> str:\n",
        "    if input_ctxt:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input_ctxt}\n",
        "\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "vT036U9nbXbJ"
      },
      "id": "vT036U9nbXbJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "822d5771-44b8-4f31-983a-5f7da46e37a5",
      "metadata": {
        "id": "822d5771-44b8-4f31-983a-5f7da46e37a5"
      },
      "outputs": [],
      "source": [
        "def generate_response(instruction: str, input_ctxt, generation_config) -> str:\n",
        "    prompt = generate_prompt(instruction, input_ctxt)\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ce6c13-1e0c-4d29-a65a-022a99d621bb",
      "metadata": {
        "tags": [],
        "id": "08ce6c13-1e0c-4d29-a65a-022a99d621bb",
        "outputId": "ff7dfd69-c86a-4989-9288-b1a502903043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Write a poem about Berlin\n",
            "\n",
            "### Response:\n",
            "Verse 1\n",
            "\n",
            "The streets of Berlin are filled with life,\n",
            "A city of culture and of strife.\n",
            "From the Brandenburg Gate to the Reichstag,\n",
            "Berlin is a place of great delight.\n",
            "\n",
            "Verse 2\n",
            "\n",
            "The Tiergarten is a place of beauty,\n",
            "A place of peace and tranquility.\n",
            "From the Victory Column to the Holocaust Memorial,\n",
            "Berlin is a city of many wonders.\n",
            "\n",
            "Verse 3\n",
            "\n",
            "The Berlin Wall is a reminder of the past,\n",
            "A reminder of a divided city.\n",
            "From Checkpoint Charlie to the East Side Gallery,\n",
            "Berlin is a city of many stories.\n"
          ]
        }
      ],
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature=0,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "\n",
        "instruction = \"Write a poem about Berlin\"\n",
        "input_ctxt = None\n",
        "\n",
        "response = generate_response(instruction, input_ctxt, generation_config)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}